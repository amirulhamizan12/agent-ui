<div align="center">

<img width="1661" height="403" alt="Browser Use" src="https://github.com/user-attachments/assets/a56f8da9-cbe3-46ed-a0c2-a60606609c6f" />

---

# Realtime Studio â€” Voice-Driven Browserâ€‘Use Agent (for #nicehack69)

---

**Speech â†’ Web â†’ Done.**  
*Sets a new bar for Browserâ€‘Use agents: real control, real artifacts, real time â€” PRs, bookings, filings â€” not another chat demo.*

---

[![Browser-Use](https://img.shields.io/badge/Browser--Use-3.0-blue?style=flat-square&logo=github)](https://github.com/browser-use/browser-use)
[![Gemini AI](https://img.shields.io/badge/Gemini-AI-orange?style=flat-square&logo=google)](https://ai.google.dev/)
[![Next.js](https://img.shields.io/badge/Next.js-15-black?style=flat-square&logo=next.js)](https://nextjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.0-blue?style=flat-square&logo=typescript)](https://www.typescriptlang.org/)

</div>

## ğŸŒŸ The 30â€‘second pitch

You speak. The agent navigates the web through Browserâ€‘Use, clicks buttons, fills forms, scrapes results, and returns artifacts you can share â€” PRs, GitHub comments, bookings, and reports. Itâ€™s a voiceâ€‘first control plane for the browser that turns "do this online" into done.

## ğŸš€ What we built (and why it matters)

Realtime Studio is a productionâ€‘quality, voiceâ€‘driven Browserâ€‘Use agent. It blends live speech understanding (Gemini) with real browser control (Browserâ€‘Use v2) and a clear operator UI so judges can watch the agent work, step by step, and verify outcomes.

### ğŸ¯ Core capabilities
- **Voiceâ€‘in, Actionâ€‘out**: Live mic streaming to Gemini; the model emits explicit action tags; Browserâ€‘Use executes them.
- **True browser control**: Sessions, live view, highlight elements, vision, screenshots, and resilient stop/resume.
- **Outcome artifacts**: The UI aggregates steps and output into shareable reports â€” designed for PRs/issues and demo clips.


## ğŸ¬ Flagship demos (judgeâ€‘friendly and viralâ€‘ready)

### 1) Accessibility Fixer Agent â†’ audits and opens a PR
- You say: "Audit `example.com` and fix lowâ€‘hanging issues."
- Agent does: navigate a11y tools, capture issues, fork repo via GitHub web, apply simple fixes (alt/aria/heading), open a **PR**, attach a **report** with screenshots.
- Judge sees: live steps, a clean report, and a real PR link.

### 2) Bug Reproducer & Issue Filer â†’ reproduces, records, comments
- You say: "Reproduce this GitHub issue and post evidence." (paste issue URL)
- Agent does: follow steps, record console/network + screenshots, compile a **markdown report**, and **comment back on the issue** with artifacts.
- Judge sees: reliable repro, clear evidence, and a live issue comment link.

## ğŸ”® The Bigger Picture: Towards General AI Agents

This project is just the **beginning**. Inspired by groundbreaking AI agents like [Manus AI](https://www.manusai.io) and [Loveable](https://loveable.dev), we're building toward something much bigger:

### **Phase 1: Realtime Studio** âœ…
*Current implementation - voice commands for web automation*

### **Phase 2: Multi-Platform AI Agent** ğŸš§
*Integration with mobile apps, desktop software, and IoT devices*

### **Phase 3: General Purpose AI Assistant** ğŸ”®
*Autonomous task planning and execution across all digital platforms*

### **The Vision: Your Personal AI Co-Pilot**
Imagine an AI that doesn't just respond to commands, but **anticipates your needs**:

- **Proactive Assistance**: "I noticed you're researching laptops - here are 3 deals that just went live"
- **Cross-Platform Intelligence**: "I've updated your calendar, booked your flight, and sent the confirmation to your team"
- **Learning & Adaptation**: "Based on your preferences, I've found a new restaurant you'll love"

## ğŸ† Why this wins #nicehack69

- **Dreams big, delivers now**: Voiceâ€‘first assistants are inevitable; this one already turns speech into verifiable web outcomes.
- **Core to Browserâ€‘Use**: Sessions, tasks, live control, and step telemetry showcase the SDKâ€™s strengths (and surface feedback).
- **No bullshit**: If a text model alone could do it, we didnâ€™t build it. Every demo ends with a shareable artifact (PR, issue, or report).
- **Judgeâ€‘ready UX**: Live iframe, step gallery, status indicators, and exportable results make the demo obvious and compelling.

## ğŸ›  How it works (under the hood)

### **Architecture Overview**
- **Frontend**: Next.js 15 with TypeScript and Tailwind CSS
- **Speech + Text**: Gemini 2.0 Flash Live over WebSockets for lowâ€‘latency input/output
- **Browser Automation**: Browserâ€‘Use API v2 for intelligent web interaction
- **Audio Processing**: Custom Web Audio API worklets for realâ€‘time streaming
- **State Management**: React Context with sophisticated task orchestration

### **Key Features**
- ğŸ¤ **Real-time Voice Input**: Continuous speech recognition with visual feedback
- ğŸ§  **Intelligent Action Parsing**: AI-powered command interpretation and task planning
- ğŸŒ **Live Browser Automation**: Real-time web interaction with step-by-step visualization
- ğŸ”Š **Natural Voice Responses**: High-quality text-to-speech with multiple voice options
- ğŸ“± **Responsive Design**: Seamless experience across desktop and mobile devices
- âš¡ **Performance Optimized**: Efficient WebSocket connections and audio streaming

### **Browser-Use Integration**
Our implementation showcases Browserâ€‘Use's full potential:
- **Session Management**: Persistent browser sessions for complex multiâ€‘step tasks
- **Realâ€‘time Monitoring**: Live view of automation progress with stepâ€‘byâ€‘step breakdown
- **Error Handling**: Robust error recovery and user feedback systems
- **Scalable Architecture**: Designed to handle concurrent users and complex workflows

## ğŸš€ Getting Started

### **Prerequisites**
- Node.js 18+ 
- npm or yarn
- Gemini AI API key
- Browser-Use API key

### **Installation**
```bash
# Clone the repository
git clone https://github.com/yourusername/realtime-studio-hackathon.git
cd realtime-studio-hackathon

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env.local
# Add your API keys to .env.local

# Start the development server
npm run dev
```

### **Environment Variables**
```env
NEXT_PUBLIC_GEMINI_API_KEY=your_gemini_api_key
NEXT_PUBLIC_BROWSER_USE_API_KEY=your_browser_use_api_key
```

## ğŸ¯ What judges will see in 2 minutes

1) You say a task out loud.
2) The agent opens a live browser view and performs the steps (clicks, forms, navigation, screenshots).
3) A tangible artifact appears: a PR link, an issue comment, or a polished report you can share on X.

## ğŸ“Š Benchmarks that matter (for live demos)

- **Speech latency**: typically < 200ms
- **Action cadence**: ~1â€“2s/step in Browserâ€‘Use live sessions
- **Result time**: < 10s for common multiâ€‘step flows

## ğŸ¤ Contributing and feedback

This project was built for #nicehack69. We want your toughest sites, your worst UX, and your bug reports. Open an issue, share a URL, or propose a demo challenge â€” the agent should handle it.

Areas to contribute:
- Voice command patterns and action prompting
- Robustness across tricky UIs (iframes, shadow DOM, virtualized lists)
- Artifact exporters (GitHub PR/Issue helpers, report generators, clip makers)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Browserâ€‘Use Team** â€” the core engine that turns intent into action
- **Google Gemini** â€” lowâ€‘latency speech and control
- **#nicehack69 Community** â€” for the challenge and the inspiration

---

## ğŸ Submission

- X demo post: [link coming at submission]
- GitHub issue in the Browserâ€‘Use repo: [link coming at submission]
- Public live demo URL: [coming soon]

---

*Built with â¤ï¸ for #nicehack69 â€” celebrating 69,000 GitHub stars*
